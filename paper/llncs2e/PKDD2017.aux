\relax 
\citation{1}
\citation{2}
\citation{3}
\citation{2}
\citation{5}
\citation{4}
\citation{2}
\citation{4}
\citation{7}
\citation{8}
\citation{9}
\citation{2}
\citation{4}
\@writefile{toc}{\contentsline {title}{Learning the local coding scheme: Jointly Optimized Locally Linear Classifiers }{1}}
\@writefile{toc}{\authcount {5}}
\@writefile{toc}{\contentsline {author}{Teng Zhang \unskip {} \and Chenghao Liu \unskip {} \and Peilin Zhao \unskip {} \and Steven C.H. Hoi \unskip {} \and Jianling Sun \unskip {}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{4}
\citation{11}
\citation{12}
\citation{9}
\citation{10}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Localized Soft-assignment Coding Scheme}{2}}
\citation{18}
\citation{19}
\citation{20}
\citation{21}
\citation{8}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Locally Linear Classifier}{3}}
\citation{12}
\citation{2}
\citation{12}
\citation{2}
\citation{4}
\citation{2}
\citation{4}
\citation{16}
\citation{17}
\@writefile{toc}{\contentsline {section}{\numberline {3}Jointly Optimized Locally Linear Classifier}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Optimized Localized Soft-assignment Coding}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Motivation for adaptively picking anchor points. In the two scenarios above, the same distribution of anchor points are given (represented by blue dots). The new data point to be estimated is shown as a red dot. Intuitively, in the left scenario it would be resonable to invole more anchor points in the local coding phase while in the right scenario considering fewer anchor points may relieve unreliable estimation of the membership of distint anchor points. In the conventional coding scheme, without loss of generality, fix $k = 8$, the surrounding anchor points range is denoted by green circle. Clearly, in the second scenairo, involving distant anchor points may lead to unreliable estimation of final prediction.}}{6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{6}}
\citation{4}
\citation{6}
\citation{4}
\citation{4}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Optimized Localized Soft-assignment Coding Algorithm}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Stochastic Gradient Descent Algorithm for LLC-JO}{8}}
\citation{13}
\citation{14}
\citation{13}
\citation{15}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Joint optimized Locally Linear Classifier (LLC-JO)}}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experimental Setup}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Datasets \& Evaluation Metrics.}{10}}
\citation{2}
\citation{4}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Basic statistics of datasets}}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Model Comparsion.}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Parameter Settings.}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experimental Results \& Analysis}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation of Parameter Sensitivity}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of different algorithms in terms of train loss, test loss, classification accuracy and test time (normalized to test time of SVM)}}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Epoch-wise demonstration of different algorithms with hinge loss on test data}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {phishing}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Magic04}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {IJCNN}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {w8a}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {connect-4}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Covtype}}}{14}}
\newlabel{allfig}{{2}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{14}}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{4}{4}
\bibcite{5}{5}
\bibcite{6}{6}
\bibcite{7}{7}
\bibcite{8}{8}
\bibcite{9}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Epoch-wise demonstration of LLC-DJO nn learning curve on test data}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {phishing}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Magic04}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {IJCNN}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {w8a}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {connect-4}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Covtype}}}{15}}
\newlabel{allfig}{{3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Sensitivity of Lipschitz to noise ratio on dataset Magic04. The red line indicates the average number of nearest neighbors in test data while the blue line denotes the test hingle loss in terms of different value of $\mu $. The black line denotes the corresponding test hinge loss of LLC-SAPL baseline.}}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces anchor points number vs hingeloss}}{16}}
\bibcite{10}{10}
\bibcite{11}{11}
\bibcite{12}{12}
\bibcite{13}{13}
\bibcite{14}{14}
\bibcite{15}{15}
\bibcite{16}{16}
\bibcite{17}{17}
\bibcite{18}{18}
\bibcite{19}{19}
\bibcite{20}{20}
\bibcite{21}{21}
