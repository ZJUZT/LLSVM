\relax 
\citation{1}
\citation{2}
\citation{3}
\citation{2}
\citation{5}
\citation{4}
\citation{2}
\citation{4}
\citation{7}
\citation{8}
\citation{9}
\citation{2}
\citation{4}
\@writefile{toc}{\contentsline {title}{Learning the local coding scheme: Jointly Optimized Locally Linear Classifiers }{1}}
\@writefile{toc}{\authcount {5}}
\@writefile{toc}{\contentsline {author}{Teng Zhang \unskip {} \and Chenghao Liu \unskip {} \and Peilin Zhao \unskip {} \and Steven C.H. Hoi \unskip {} \and Jianling Sun \unskip {}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{4}
\citation{11}
\citation{12}
\citation{9}
\citation{10}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Localized Soft-assignment Coding Scheme}{2}}
\citation{18}
\citation{19}
\citation{20}
\citation{21}
\citation{8}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Locally Linear Classifier}{3}}
\citation{12}
\citation{2}
\citation{12}
\citation{2}
\citation{4}
\citation{2}
\citation{4}
\citation{16}
\citation{17}
\@writefile{toc}{\contentsline {section}{\numberline {3}Jointly Optimized Locally Linear Classifier}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Optimized Localized Soft-assignment Coding}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Motivation for adaptively picking anchor points. In the two scenarios above, the same distribution of anchor points are given (represented by blue dots). The new data point to be estimated is shown as a red dot. Intuitively, in the left scenario it would be resonable to invole more anchor points in the local coding phase while in the right scenario considering fewer anchor points may relieve unreliable estimation of the membership of distint anchor points. In the conventional coding scheme, without loss of generality, fix $k = 8$, the surrounding anchor points range is denoted by green circle. Clearly, in the second scenairo, involving distant anchor points may lead to unreliable estimation of final prediction.\relax }}{6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{6}}
\citation{4}
\citation{6}
\citation{4}
\citation{4}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Optimized Localized Soft-assignment Coding Algorithm\relax }}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Stochastic Gradient Descent Algorithm for LLC-JO}{8}}
\citation{13}
\citation{14}
\citation{13}
\citation{15}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Joint optimized Locally Linear Classifier (LLC-JO)\relax }}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{10}}
\citation{2}
\citation{4}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experimental Setup}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Datasets \& Evaluation Metrics.}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Basic statistics of datasets\relax }}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Model Comparsion.}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Parameter Settings.}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experimental Results \& Analysis}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of different algorithms in terms of train loss, test loss, classification accuracy and test time (normalized to test time of SVM)\relax }}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Cumulative Hinge Loss in learning process on Magic04 and IJCNN dataset.\relax }}{13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Magic04}}}{13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {IJCNN}}}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cumulative Average Number of Nearest Neighbors in learning process on Magic04 and IJCNN dataset.\relax }}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Magic04}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {IJCNN}}}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Sensitivity of Lipschitz to noise ratio on dataset Magic04. The red line indicates the average number of nearest neighbors in test data while the blue line denotes the test hingle loss in terms of different value of $\mu $. The black line denotes the corresponding test hinge loss of LLC-SAPL baseline.\relax }}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation of Parameter Sensitivity}{14}}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{4}{4}
\bibcite{5}{5}
\bibcite{6}{6}
\bibcite{7}{7}
\bibcite{8}{8}
\bibcite{9}{9}
\bibcite{10}{10}
\bibcite{11}{11}
\bibcite{12}{12}
\bibcite{13}{13}
\bibcite{14}{14}
\bibcite{15}{15}
\bibcite{16}{16}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{15}}
\bibcite{17}{17}
\bibcite{18}{18}
\bibcite{19}{19}
\bibcite{20}{20}
\bibcite{21}{21}
